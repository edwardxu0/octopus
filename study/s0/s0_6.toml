name = 's0'

[train]
	artifact = 'MNIST'
	net_name = 'NetS'
	net_layers = [128, 128, 128]		# hidden layers

	epochs = 100
	batch_size = 64
	test_batch_size = 1000
	lr = 1e-4							# learning rate
	gamma = 0.95 							# Learning rate decay
	save_model = true
	save_intermediate = true
	save_log = true
	log_interval = 10					# Stable ReLU estimation/print interval
	gpu = true
	amp = true							# enabled only when gpu == true

	# TD: train distribution
	# VS: verification sampling
	# IP: interval propagation
	ReLU_estimation = 'TD'
	adv_train = 'nature' 				# ['nature', 'pgd', 'vae', 'gan', ...]

[heuristic]
	[heuristic.bias_shaping]
		ReLU_estimation = 'TD'
		mode = 'standard'
		intensity = 5e-2
		occurrence = 5e-3
		#every = 100
		decay = 0.99
		start = 1 			# inclusive
		end = 100 			# inclusive
	
	#[heuristic.rs_loss]
	#	ReLU_estimation = 'IP'
	#	mode = 'standard'
	#	weight = 1e-4
	#	epsilon = 1e-1
	#	start = 1
	#	end = 9

	#[heuristic.prune]
	#	mode = 'structure'
	#	re_arch = 'standard'
	#	sparsity = 0.05
	#	start = [1,2,3,4,5,6,7,8,9,10]
	#	end = [1,2,3,4,5,6,7,8,9,10]


[verify]
	time = 600
	memory = '8G'
	property = 0
	epsilon = 0.02
	debug = false
	save_log = true
	verifier = 'neurify'
	
	# model selection strategy for verification
	# last(default): last epoch model
	#
	# For all below: must save intermediate models(save_intermediate = true)
	# best test accuracy: model with the best test accuracy
	# best relu accuracy: model with the most amount of stable relus
	# top [x] test accuaracy: model with the most amount of stable relus with [x]% accuarcy difference of the model with the best tests accuracy
	# top [x] relu accuaracy: model with the best test accuracy with [x]% stable relu difference of the model with the most amount of stable relus
	target_model = 'top 1 test accuracy'

